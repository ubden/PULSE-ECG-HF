# PULSE-7B Handler Deployment Guide

## ğŸš€ Deployment Rehberi

### Gereksinimler
- Python 3.8+
- CUDA 11.8+ (GPU kullanÄ±mÄ± iÃ§in)
- Minimum 16GB RAM (CPU), 8GB VRAM (GPU)

### Kurulum

1. **BaÄŸÄ±mlÄ±lÄ±klarÄ± yÃ¼kleyin:**
```bash
pip install -r requirements.txt
```

2. **LLaVA Architecture DesteÄŸi (PULSE-7B iÃ§in kritik):**
```bash
# EÄŸer "llava_llama architecture not recognized" hatasÄ± alÄ±rsanÄ±z:
pip install --upgrade transformers

# Veya en son development sÃ¼rÃ¼mÃ¼:
pip install git+https://github.com/huggingface/transformers.git
```

3. **Flash Attention (isteÄŸe baÄŸlÄ±, performans iÃ§in):**
```bash
pip install flash-attn --no-build-isolation
```

### HuggingFace Inference Deployment

#### 1. Model Repository YapÄ±sÄ±
```
your-model-repo/
â”œâ”€â”€ handler.py
â”œâ”€â”€ config.json
â”œâ”€â”€ generation_config.json
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ model.safetensors.index.json
â”œâ”€â”€ tokenizer_config.json
â”œâ”€â”€ special_tokens_map.json
â””â”€â”€ tokenizer.model
```

#### 2. Endpoint OluÅŸturma
```bash
# HuggingFace CLI ile deploy
huggingface-cli login
huggingface-cli repo create your-pulse-endpoint --type=space
```

#### 3. Test Requests

**Image URL ile test:**
```bash
curl -X POST "YOUR_ENDPOINT_URL" \
  -H "Content-Type: application/json" \
  -d '{
    "inputs": {
      "query": "Analyze this ECG image",
      "image": "https://i.imgur.com/7uuejqO.jpeg"
    },
    "parameters": {
      "temperature": 0.2,
      "max_new_tokens": 512
    }
  }'
```

**Base64 ile test:**
```bash
curl -X POST "YOUR_ENDPOINT_URL" \
  -H "Content-Type: application/json" \
  -d '{
    "inputs": {
      "query": "What do you see in this ECG?",
      "image": "data:image/jpeg;base64,/9j/4AAQ..."
    },
    "parameters": {
      "temperature": 0.2
    }
  }'
```

### Performans OptimizasyonlarÄ±

#### GPU Memory Optimizasyonu
- `torch_dtype=torch.bfloat16` kullanÄ±n
- `low_cpu_mem_usage=True` ayarlayÄ±n
- `device_map="auto"` ile otomatik daÄŸÄ±tÄ±m

#### CPU Optimizasyonu
- `torch_dtype=torch.float32` kullanÄ±n
- Thread sayÄ±sÄ±nÄ± ayarlayÄ±n: `torch.set_num_threads(4)`

### Monitoring ve Debugging

#### Log Seviyeleri
```python
import logging
logging.basicConfig(level=logging.INFO)
```

#### Memory Usage
```python
import torch
print(f"GPU Memory: {torch.cuda.memory_allocated()/1024**3:.2f}GB")
```

### Troubleshooting

#### Common Issues:

1. **"llava_llama architecture not recognized" Error**
   ```bash
   # PULSE-7B Solution: Install PULSE's LLaVA implementation
   pip install git+https://github.com/AIMedLab/PULSE.git#subdirectory=LLaVA
   
   # Also install development transformers
   pip install git+https://github.com/huggingface/transformers.git
   
   # Or add both to requirements.txt:
   git+https://github.com/huggingface/transformers.git
   git+https://github.com/AIMedLab/PULSE.git#subdirectory=LLaVA
   ```

2. **CUDA Out of Memory**
   - Batch size'Ä± azaltÄ±n
   - `max_new_tokens` deÄŸerini dÃ¼ÅŸÃ¼rÃ¼n
   - Gradient checkpointing kullanÄ±n

3. **Slow Image Processing**
   - Image timeout deÄŸerini artÄ±rÄ±n
   - Image resize threshold ayarlayÄ±n

4. **Model Loading Issues**
   - HuggingFace token'Ä±nÄ± kontrol edin
   - Network baÄŸlantÄ±sÄ±nÄ± doÄŸrulayÄ±n
   - Cache dizinini temizleyin
   - Transformers sÃ¼rÃ¼mÃ¼nÃ¼ kontrol edin

### Security Best Practices

- Image URL'leri validate edin
- Base64 boyut limitlerini ayarlayÄ±n
- Rate limiting uygulayÄ±n
- Input sanitization yapÄ±n

### Monitoring Metrics

- Response time
- Memory usage
- Error rates
- Image processing success rate
- Token generation speed
